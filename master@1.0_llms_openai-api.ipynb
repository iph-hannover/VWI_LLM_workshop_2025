{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs - Using the OpenAI API\n",
    "by Tobias Heimig-Elschner (BBSR) - 2025\n",
    "\n",
    "In this notebook, we will use the OpenAI Python client library to interact with the OpenAI API.\n",
    "We will establish a connection to the API and use it to:\n",
    "- Get embeddings for a given text\n",
    "- Perform single-turn completions\n",
    "- Create multi-turn conversations\n",
    "\n",
    "### Pre-requisites\n",
    "- OpenAI API key (the API key should be provided as an environment variable OPENAI_API_KEY) \n",
    "- **NOTE**: API keys are sensitive credentials and should never be hard-coded or shared. They should be stored securely and accessed programmatically.\n",
    "- Python client library for the OpenAI API `openai`\n",
    "\n",
    "### Further Reading\n",
    "- [OpenAI Cookbook](https://cookbook.openai.com/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/introduction)\n",
    "- [OpenAI Python Client Library README](https://github.com/openai/openai-python/blob/main/README.md)\n",
    "- [OpenAI Python Client Library API Reference](https://github.com/openai/openai-python/blob/main/api.md)\n",
    "- [OpenAI Python Client Library Examples](https://github.com/openai/openai-python/tree/main/examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This section shows how to set up the OpenAI client for interacting with the OpenAI API.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI Python library (`openai`) to create an OpenAI client instance with `openai.Client`.\n",
    "\n",
    "### Tips\n",
    "- Use the `openai.Client` class to create a client object.\n",
    "- The API key can be accessed using the environment variable `OPENAI_API_KEY` with  `os.getenv('OPENAI_API_KEY')`\n",
    "- Providing the API key is optional, because the client uses the `OPENAI_API_KEY` environment variable by default if no key is explicitly passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# OPTIONAL: Load OpenAI API key manually (default behavior already reads from env)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Get an openai-client\n",
    "ai_client = openai.Client() # openai.Client(api_key) if you want to set it explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Embedding Models - Vectorization of Data\n",
    "\n",
    "In this section, we will use the OpenAI API to get embeddings for a given text - get the embedding of 'OECD'.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API Endpoint `embeddings` to get the embeddings for 'OECD'.\n",
    "\n",
    "### Tips\n",
    "- Use the `Client.embeddings.create` method.\n",
    "- Use the model: 'text-embedding-3-small' as the embedding model.\n",
    "- Specify the text you want to embed as the `input` variable in the `Client.embeddings.create` method.\n",
    "- Specify the model using the `model` variable in the `Client.embeddings.create` method.\n",
    "- The embeddings are returned as a list of floats and is returned in a `CreateEmbeddingResponse` object.\n",
    "    - The embeddings can be accessed using the `CreateEmbeddingResponse.data` attribute which contains the list of objects.\n",
    "    - The first element of the list is the actual embedding object.\n",
    "    - The embedding (a list of floats) can be accessed using the `CreateEmbeddingResponse.data[0].embedding` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model and text\n",
    "model = \"text-embedding-3-small\"\n",
    "text = \"OECD\"\n",
    "\n",
    "# Query the embedding\n",
    "response = ai_client.embeddings.create(\n",
    "    model = model,\n",
    "    input = text\n",
    ")\n",
    "\n",
    "#Getting the embedding\n",
    "embedding = response.data[0].embedding\n",
    "print(f\"Embedding: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further information - Inspecting `CreateEmbeddingResponse`\n",
    "- `CreateEmbeddingResponse.data` &rarr; a list of embeddings (`Embedding`); list because the API endpoints allows to create multiple embeddings in one request as long as the number of max tokens is not exceeded.\n",
    "- `CreateEmbeddingResponse.model` &rarr; the name (`str`) of the model used\n",
    "- `CreateEmbeddingResponse.object` &rarr; the type (`str`) of the data-object that is returned for the request (for embeddings-endpoint always 'list')\n",
    "- `CreateEmbeddingResponse.usage` &rarr; a usage-obj(`Usage`) containing information about the used tokens (prompt_tokens and total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Chat Models - Single Turn\n",
    "\n",
    "In this section, we will use the OpenAI API to get a single-turn completion for a given user prompt.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API endpoint `completions` to get a single-turn completion for a given user prompt – \"What does 'OECD' stand for?\"\n",
    "\n",
    "### Tips\n",
    "- Use the `Client.chat.completions.create` method.\n",
    "- Use the `'gpt-4o-mini-2024-07-18'` model to get the completions.\n",
    "- Provide the model as `'model'` variable in the `Client.chat.completions.create` method.\n",
    "- Provide the text to specify as prompt as a list of dictionaries with the keys `'role'` and `'content'` in the `Client.chat.completions.create` method.\n",
    "    - In this example we only provide a single message with the user prompt.\n",
    "    - The `'role'` key should contain `'user'` for user prompt.\n",
    "    - The `'content'` key should contain the actual prompt as a string.\n",
    "- The answer/chat-completion is returned as a `ChatCompletion` object.\n",
    "    - The answer can be accessed using the `ChatCompletion.choices[0].message` attribute, which contains the answer as a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for the request\n",
    "role = \"user\"\n",
    "content = \"What does OECD stand for?\"\n",
    "messages = [{\"role\": role, \"content\": content}]\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "# Request completion from openai_api\n",
    "completion = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Extract the answer from the completion\n",
    "answer = completion.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further information – inspecting `ChatCompletion`\n",
    "\n",
    "- `ChatCompletion.id` → a unique identifier (`str`) for the chat completion.\n",
    "- `ChatCompletion.choices` → a list of `Choice` objects; each `Choice` contains:\n",
    "    - `Choice.finish_reason` → the reason (`str`) why the completion stopped (e.g., 'stop').\n",
    "    - `Choice.index` → the index (`int`) of the choice in the list.\n",
    "    - `Choice.message` → the actual message (`ChatCompletionMessage`) returned by the assistant.\n",
    "        - `ChatCompletionMessage.content` → the content (`str`) of the assistant's response.\n",
    "        - `ChatCompletionMessage.role` → the role (`str`) of the message sender (e.g., 'assistant').\n",
    "- `ChatCompletion.created` → the timestamp (`int`) when the completion was created.\n",
    "- `ChatCompletion.model` → the name (`str`) of the model used for the completion.\n",
    "- `ChatCompletion.object` → the type (`str`) of the returned object (e.g., 'chat.completion').\n",
    "- `ChatCompletion.service_tier` → the service tier (`str`) used for the request.\n",
    "- `ChatCompletion.system_fingerprint` → a unique fingerprint (`str`) for the system.\n",
    "- `ChatCompletion.usage` → a `CompletionUsage` object containing token usage details:\n",
    "    - `CompletionUsage.completion_tokens` → the number of tokens (`int`) in the completion.\n",
    "    - `CompletionUsage.prompt_tokens` → the number of tokens (`int`) in the prompt.\n",
    "    - `CompletionUsage.total_tokens` → the total number of tokens (`int`) used.\n",
    "    - `CompletionUsage.completion_tokens_details` → a `CompletionTokensDetails` object with detailed token usage for the completion.\n",
    "    - `CompletionUsage.prompt_tokens_details` → a `PromptTokensDetails` object with detailed token usage for the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Chat Models - Stream\n",
    "\n",
    "In this section, we will use the streaming feature of the OpenAI API to get the answer bit by bit while it is generated.  \n",
    "We will use the OpenAI API again to get a single answer completion for a given text – \"What does 'OECD' stand for?\".\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API endpoint `completions` to get a single answer completion for the text – \"What does 'OECD' stand for?\" using the streaming feature.\n",
    "\n",
    "### Tips\n",
    "- Use the `Client.chat.completions.create` method as before.\n",
    "- Use the `'gpt-4o-mini-2024-07-18'` model to get the completions.\n",
    "- Set the `stream` parameter to `True` in the `Client.chat.completions.create` method.\n",
    "- To print the answer bit by bit, use a for-loop to iterate over the `Stream` object (e.g., `for chunk in stream`) returned by the `Client.chat.completions.create` method.\n",
    "    - The answer can be accessed via `chunk.choices[0]`, which contains the current state of the answer.\n",
    "    - Use `chunk.choices[0].message.delta` to get only the new part of the answer.\n",
    "    - Use `chunk.choices[0].message.delta.content` to get the new part of the answer as a string.\n",
    "- In order to print inline, provide the `end=''` parameter to the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for the request\n",
    "role = \"user\"\n",
    "content = \"What does OECD stand for?\"\n",
    "messages = [{\"role\": role, \"content\": content}]\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "# Request completion from openai_api\n",
    "stream = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "for chunk in stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Chat Models - Multi Turn\n",
    "\n",
    "In this section, we will use the OpenAI API to get multiple completions for a given text — conversation-like.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API endpoint `completions` to get multiple completions.\n",
    "- In a first step, we will provide the prompt \"What does 'OECD' stand for?\", and in a second step, we will provide the prompt \"What is the purpose of it?\".\n",
    "\n",
    "### Tips\n",
    "- Use the `Client.chat.completions.create` method as before.\n",
    "- Use a `history` variable to store the previous messages. Messages from both the user and the assistant are stored in this variable.\n",
    "- Use the `'gpt-4o-mini-2024-07-18'` model to get the completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for the requests\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "history = []\n",
    "\n",
    "# Prepare the first message\n",
    "message_1 = {\"role\": \"user\", \"content\": \"What does 'OECD' stand for?\"}\n",
    "history.append(message_1) # Append first message to history\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "      \n",
    "# Request first completion from openai_api\n",
    "completion = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history\n",
    ")\n",
    "\n",
    "# Extract the answer from the completion\n",
    "answer_1 = completion.choices[0].message\n",
    "history.append(answer_1.to_dict())\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "\n",
    "# Request second completion from openai_api\n",
    "message_2 = {\"role\": \"user\", \"content\": \"What is the purpose of it?\"}\n",
    "history.append(message_2) # Append second message to history\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "\n",
    "# Prepare the second message\n",
    "completion = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history\n",
    ")\n",
    "\n",
    "# Extract the answer from the completion\n",
    "answer_2 = completion.choices[0].message\n",
    "history.append(answer_2.to_dict())\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Chat Models - System Prompt\n",
    "\n",
    "In this section, we will use the OpenAI API to get multiple completions for a given text as before, but this time we will provide a system prompt to give some context about the OECD and ensure that the model answers only questions that are within the context of the OECD.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API endpoint `completions` to get multiple completions as before.\n",
    "- Use a system prompt to provide some context about the OECD and make sure that the model only answers questions in the context of the OECD.\n",
    "- Try what happens if you ask a question that is not in the context of the OECD with the system prompt.\n",
    "\n",
    "### Tips\n",
    "- Use the `Client.chat.completions.create` method as before.\n",
    "- As the first message in the `messages` list, provide **always** the system prompt; the `messages` list is passed as input to the `Client.chat.completions.create` method.\n",
    "    - The `'role'` key should contain `'system'` for the system prompt.\n",
    "    - The `'content'` key should contain the actual system prompt as a string.\n",
    "- As the second message in the `messages` list, provide the user prompt.\n",
    "    - The `'role'` key should contain `'user'` for the user prompt.\n",
    "    - The `'content'` key should contain the actual user prompt as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for the requests\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "history = []\n",
    "system_prompt = {'role' : 'system',\n",
    "                 'content' : \"\"\"You are a helpful, friendly and professional assistant helping with questions about the OECD.\n",
    "                             You can only answer question about the OECD or its context. If a question is outside of this scope, please let the user know that you can't help with that.\n",
    "                             You can also provide information about the OECD's purpose, history, members, etc. If you don't know the answer, you can say that you don't know.\n",
    "                             Here are some important facts about the OECD:\n",
    "                                - The OECD stands for the Organization for Economic Co-operation and Development.\n",
    "                                - The OECD was founded in 1961 to stimulate economic progress and world trade.\n",
    "                                - The OECD has 38 member countries, including the United States, the United Kingdom, and Japan.\n",
    "                                - The OECD is headquartered in Paris, France.\n",
    "                                - The OECD publishes reports and statistics on a wide range of economic and social issues.\n",
    "                 \"\"\"}\n",
    "\n",
    "history.append(system_prompt)\n",
    "\n",
    "# Prepare the first message\n",
    "message_1 = {\"role\": \"user\", \"content\": \"What does 'OECD' stand for?\"}\n",
    "history.append(message_1) # Append first message to history\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "      \n",
    "# Request first completion from openai_api\n",
    "completion = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history\n",
    ")\n",
    "\n",
    "# Extract the answer from the completion\n",
    "answer_1 = completion.choices[0].message\n",
    "history.append(answer_1.to_dict())\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "\n",
    "# Request second completion from openai_api out of scope question\n",
    "message_2 = {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "history.append(message_2) # Append second message to history\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')\n",
    "\n",
    "# Prepare the second message\n",
    "completion = ai_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history\n",
    ")\n",
    "\n",
    "# Extract the answer from the completion\n",
    "answer_2 = completion.choices[0].message\n",
    "history.append(answer_2.to_dict())\n",
    "print(f'{history[-1][\"role\"]} :: \\n{history[-1][\"content\"]}\\n---\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple Chatbot\n",
    "\n",
    "In this section, we will build a simple chatbot that can answer questions about the OECD. The chatbot uses the OpenAI API and incorporates a **system prompt** to define its behavior and scope. It will **stream responses** in real time and maintain context across multiple turns in the conversation.\n",
    "\n",
    "### Task\n",
    "- Use the OpenAI API to build a chatbot that responds to user questions.\n",
    "- Define a system prompt that restricts the chatbot to only answer questions related to the OECD.\n",
    "- Accept continuous user input and maintain a history of the conversation.\n",
    "- Use streaming to display the response in real time.\n",
    "- End the chat by typing `\"break\"`.\n",
    "\n",
    "### Tips\n",
    "- **Set a system prompt** as the first message in the `history` list. This defines the chatbot’s personality, scope (OECD-related), and behavior when asked questions outside its domain.\n",
    "- **Initialize an empty `history` list** and append user and assistant messages as the conversation continues.\n",
    "- Use `input()` to capture user input. Check if the input is `\"break\"` to end the session and clear the history.\n",
    "- Call `Client.chat.completions.create()` with:\n",
    "  - `model`: e.g., `\"gpt-4o-mini-2024-07-18\"`\n",
    "  - `messages`: the `history` list containing the conversation so far\n",
    "  - `stream=True`: to enable real-time response streaming\n",
    "- Store the full assistant response in a dictionary like `{'role': 'assistant', 'content': ...}` and append it to the `history` list after the message is fully streamed.\n",
    "- **Maintaining context**: The full `history` list is sent with each new request, enabling the assistant to answer based on previous turns.\n",
    "- **Exit and reset**: When the user types `\"break\"`, print a goodbye message and clear the `history` list to reset the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chatbot data\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "history = []\n",
    "system_prompt = {'role' : 'system',\n",
    "                 'content' : \"\"\"You are a helpful, friendly and professional assistant helping with questions about the OECD.\n",
    "                             You can only answer question about the OECD or its context. If a question is outside of this scope, please let the user know that you can't help with that.\n",
    "                             You can also provide information about the OECD's purpose, history, members, etc. If you don't know the answer, you can say that you don't know.\n",
    "                             Here are some important facts about the OECD:\n",
    "                                - The OECD stands for the Organization for Economic Co-operation and Development.\n",
    "                                - The OECD was founded in 1961 to stimulate economic progress and world trade.\n",
    "                                - The OECD has 38 member countries, including the United States, the United Kingdom, and Japan.\n",
    "                                - The OECD is headquartered in Paris, France.\n",
    "                                - The OECD publishes reports and statistics on a wide range of economic and social issues.\n",
    "                 \"\"\"}\n",
    "\n",
    "history.append(system_prompt)\n",
    "\n",
    "# Initialize the chatbot\n",
    "while True:\n",
    "\n",
    "    # Wait for user input\n",
    "    user_input = input(\"You :: \\n\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "    # Check if the user wants to break the chat\n",
    "    if(user_input.lower() == \"break\"):\n",
    "        print(\"Stopped chat and cleaned history.\")\n",
    "        history = []\n",
    "        break\n",
    "\n",
    "    # Prepare the user message\n",
    "    message = {\"role\": \"user\", \"content\": user_input}\n",
    "    history.append(message) # Append user message to history\n",
    "\n",
    "    # Request completion from openai_api\n",
    "    completion = ai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Stream the response and extract the answer from the completion\n",
    "    print(\"Chatbot :: \\n\", flush=True)\n",
    "    answer = {'role' : 'assistant', 'content' : \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if not chunk.choices:\n",
    "            continue\n",
    "                   \n",
    "        if chunk.choices[0].delta.content:\n",
    "            answer[\"content\"] += chunk.choices[0].delta.content\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "            \n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "    # Add the answwer to the history\n",
    "    history.append(answer) # Append chatbot answer to history#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
